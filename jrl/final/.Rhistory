Duncan$cooks <- round(duncan.cooks,3)
# looking at rows via indexing; make sure you understand this syntax
Duncan[duncan.cooks > 4/(n-p),]
#11 REGRESSION DIAGNOSTICS III - ASSESSING LINEARITY & INFLUENCE	---------------------
# Let's construct some partial regression plots - these are nice for visualizing the (partial) slope you get in multiple regression, but are also helpful for detecting non-linearity & influential points. If there is nonlinearity, the data may move in a pattern about the linear line. If there is an influential point, it should stick out, away from the other points.
avPlots(duncan.model.1)
# Let's create a vector to be used later; all 1's except for when cooks D is large (=17)
duncan.points <- rep(1,nrow(Duncan))
duncan.points[duncan.cooks > 4/(45-3)] <- 17  #can you figure out this syntax? Give it a try!
duncan.points
# see ?points to figure out how to see what "17" means for points... its a triangle
?points
# partial regression plot showing big Cook's D's as triangles
avPlots(duncan.model.1,pch=duncan.points)
#We can also JUST plot the partial plot of prestige on income holding education constant:
?avPlots
avPlots(duncan.model.1,terms = ~income, pch=duncan.points)
#let's do the same thing as above, but do it ourselves to see exactly what's happening
#Make two windows
op <- par(mfrow=c(1,2))
# as above in the lefthand plot
avPlots(duncan.model.1,terms = ~income,pch=duncan.points)
# Now making our own partial regression plot to put in the righthand plot
income.ed <- lm(income~education)$residuals
prestige.ed <- lm(prestige~education)$residuals
plot(income.ed,prestige.ed,pch=duncan.points,main="Our partial plot")
abline(lm(prestige.ed ~ income.ed))
par(op)
#12 REGRESSION DIAGNOSTICS IV - HOMOGENEITY OF VARIANCE	---------------------
# lets plot residuals against y-hat. This will help us see constant variance and whether non-linearity exists. Here's a reminder of what we have access to
names(duncan.model.1)
#Plot residuals on fitted values. This slope is always zero by definition. We don't see much of a pattern - linearity looks OK, and I don't see much increase or decrease in spread. Regressing the fitted values on the *absolute value* of the residuals can help us see homogeneity of variance easier
plot(duncan.model.1$residuals~duncan.model.1$fitted.values)
abline(lm(duncan.model.1$residuals~duncan.model.1$fitted.values))
# this slope is NOT always zero again, we don't pick up much trend for variance to change - it increases slightly, but not much
plot(abs(duncan.model.1$residuals)~duncan.model.1$fitted.values)
abline(lm(abs(duncan.model.1$residuals)~duncan.model.1$fitted.values))
#13 REGRESSION DIAGNOSTICS V - COLINEARITY	---------------------
# the variance infation factor (vif) estimates how much variance of betas is increased by colinearity with other predictors. Because income and education are related, the variances of the betas are ~ 2x higher than if they were unrelated. vif() is another CAR function. Note: these are the same for both predictors when there are two predictors
vif(duncan.model.1)
# the sqrt of this estimates how much higher the Std. Error is than it otherwise would be
sqrt(vif(duncan.model.1))
#We can also calculate VIF by hand:
R2 <- summary(lm(education~income))$r.squared
(VIF <- 1/(1-R2))
#14 LINEAR REGRESSION II - DROPPING POINTS	---------------------
# "minister" & "conductor" professions are outliers - let's see how things change when we remove them. which() is a useful function you should put in your book of spells
not.include <- which(rownames(Duncan)=="minister" | rownames(Duncan)=="conductor")
#Check:
Duncan[not.include,] #includes them
Duncan[-not.include,] #excludes them
# this removes minister & conductor rows
duncan.model.2 <- lm(prestige~income+education,subset= -not.include)
# same thing
duncan.model.2 <- lm(prestige~income+education,data=Duncan[-not.include,])
# compare the two; note that income slope increased and education decreased. Of course, whether to drop points depends on why the outlier exists... and may change your model's external validity
summary(duncan.model.1);summary(duncan.model.2)
#15 LINEAR REGRESSION III - INTERACTIONS	---------------------
# Does the effect of income on prestige depend upon one's level of education? The syntax below says to include the two main effects + the interaction, which is specified using the : operator
duncan.model.3 <- lm(prestige~income+education+income:education)
summary(duncan.model.3)
# this is shorthand for the above in the case of two predictors. If three+ predictors, it includes ALL interactions, including three, four, etc... higher order interactions
duncan.model.3 <- lm(prestige~income*education)
summary(duncan.model.3)
# But beware - the interpretation of main (or better, "simple") effects is unclear in the presence of an interaction when main effects are uncentered. This is because the main effect of one variable is interpretted at the ZERO LEVEL of the other variable. We can help our interpretation of regression parameters in general, especially in context of interactions, by "scaling" them - i.e., transforming all variables to z-scores. The "scale" function scales the variable (mean=0, sd=1)
sd(scale(income))
mean(scale(income))
duncan.model.4 <- lm(scale(prestige)~scale(income)*scale(education))
# now the simple effects are interpretable, but must be interpreted in a new scale. We can see that there is little evidence for an income by education interaction, but that the simple effects of income and education are similar to what they were in the original model
summary(duncan.model.3);summary(duncan.model.4)
#16 LINEAR REGRESSION IV - CATEGORICAL VARIABLES	---------------------
# Now let' look at the effects of job "type" -  blue collar, professional, or white collar. Such categorical variables can be analyzed easily in regression - they just require coding schemes. In R, these types of variables are called "factors", and their coding schemes can be dictated by us although they are given default coding schemes
class(type)			# class is factor - not ordered factor. This is important for interpretation
# what is the default coding scheme R has given "type"? Remember, we can change this default using options(contrasts=xxx)
contrasts(type)
duncan.model.5 <- lm(prestige~type)   # this says to include the main effect of type
summary(duncan.model.5)	# the first effect is how much higher in prestige prof is than blue collar. The second effect is how much higher in prestige wc is than blue collar. A common mistake is to think the (e.g.) first effect is how much higher prof is than the average of bc and wc, but this isn't right. Could you have figured this out? If not, look again at contrasts(type) and ask me to explain it.
#We can look at the means of the three groups using the function tapply:
?tapply
tapply(X=prestige,INDEX=type,FUN=mean,na.rm=TRUE)
#We can change the type of default contrast by changing the factor to an ordered factor
type.ord <- ordered(type,levels=c('bc','wc','prof'))
type
Duncan
?plot
?hist
?plot
#HW Problem 1 	---------------------
# (a) Produce a histogram of the prestige variable that has 5 bins rather than 10. Title this histogram "HW1a histogram"
hist(Duncan$prestige, breaks = 9, main="Hw1a histogram")
lines(lowess(x,y),col="darkgreen",lwd=2)},   # the green lines are loess (smoothed) curves - useful for checking linearity
diag.panel = function(x){			   # diag.panel option also requires input of class "function"
par(new=TRUE)			 # par(new=TRUE) disallows hist to clear the old graphs before drawing a new histogram
#And we can add new plots to the diagonals too using "diag.panel" option
pairs(Duncan,
panel = function(x,y){ 			   # panel option requires input of class "function"
points(x,y)
abline(lm(y~x),col="red",lwd=2,lty=2)        # the red lines are linear best fits
lines(lowess(x,y),col="darkgreen",lwd=2)},   # the green lines are loess (smoothed) curves - useful for checking linearity
diag.panel = function(x){			   # diag.panel option also requires input of class "function"
par(new=TRUE)			 # par(new=TRUE) disallows hist to clear the old graphs before drawing a new histogram
hist(x,main="",axes=FALSE)}	 # creates histograms along the diagonal
)
#Rather than type all this in by hand each time, we can create a function on the fly:
MyPairs <- function(DAT){pairs(DAT,
panel = function(x,y){
points(x,y)
abline(lm(y~x),col="red",lwd=2,lty=2)
lines(lowess(x,y),col="darkgreen",lwd=2)},
diag.panel = function(x){
par(new=TRUE)
hist(x,main="",axes=FALSE)}	 )}
#Check that it works
MyPairs(Duncan)
#Check that it works
MyPairs(Duncan)
#7 ADVANCED SCATTERPLOTS OF THE DATA	---------------------
# Here is the regular old default plot using the pairs() function
pairs(Duncan)
# Let's see how we can modify the above to make a cooler graph. The rationale of the following syntax is not at all easy to see - don't worry too much right now about understanding this. It requires understanding the syntax for the "function" class - something we'll get to later in the course, but if you want to try, start with ?pairs and look at "panel" option
pairs(Duncan,
panel = function(x,y){ 			   # panel option requires input of class "function"
points(x,y)
abline(lm(y~x),col="red",lwd=2,lty=2)        # the red lines are linear best fits
lines(lowess(x,y),col="darkgreen",lwd=2)},   # the green lines are loess (smoothed) curves - useful for checking linearity
)
#And we can add new plots to the diagonals too using "diag.panel" option
pairs(Duncan,
panel = function(x,y){ 			   # panel option requires input of class "function"
points(x,y)
abline(lm(y~x),col="red",lwd=2,lty=2)        # the red lines are linear best fits
lines(lowess(x,y),col="darkgreen",lwd=2)},   # the green lines are loess (smoothed) curves - useful for checking linearity
diag.panel = function(x){			   # diag.panel option also requires input of class "function"
par(new=TRUE)			 # par(new=TRUE) disallows hist to clear the old graphs before drawing a new histogram
hist(x,main="",axes=FALSE)}	 # creates histograms along the diagonal
)
#Rather than type all this in by hand each time, we can create a function on the fly:
MyPairs <- function(DAT){pairs(DAT,
panel = function(x,y){
points(x,y)
abline(lm(y~x),col="red",lwd=2,lty=2)
lines(lowess(x,y),col="darkgreen",lwd=2)},
diag.panel = function(x){
par(new=TRUE)
hist(x,main="",axes=FALSE)}	 )}
#Check that it works
MyPairs(Duncan)
#HW Problem 1 	---------------------
# (a) Produce a histogram of the prestige variable that has 5 bins rather than 10. Title this histogram "HW1a histogram"
hist(Duncan$prestige, breaks = 9, main="Hw1a histogram")
#8 LINEAR REGRESSION	---------------------
# the function "lm" takes in an object of class "formula"
duncan.model.1 <- lm(prestige~income+education)
#7 ADVANCED SCATTERPLOTS OF THE DATA	---------------------
# Here is the regular old default plot using the pairs() function
pairs(Duncan)
#Check that it works
MyPairs(Duncan)
# (b) use the plot(), abline(), and lines() functions to create a single scatterplot (just one, not the matrix of them) exactly like the one in row 2 column 1 from the pairs() graph in #7 immediately above. Title this "HW1b scatterplot". Note that you'll need to change the mode of the variable "type" to be numeric on the fly. Also note that pairs() (which only create matrices of scatterplots) and points() (which only adds points to existing plots) won't accomplish what you want. see ?plot to figure out how to make a title on the figure.
plot(abline(lm(income~type), col = "red"))
# (b) use the plot(), abline(), and lines() functions to create a single scatterplot (just one, not the matrix of them) exactly like the one in row 2 column 1 from the pairs() graph in #7 immediately above. Title this "HW1b scatterplot". Note that you'll need to change the mode of the variable "type" to be numeric on the fly. Also note that pairs() (which only create matrices of scatterplots) and points() (which only adds points to existing plots) won't accomplish what you want. see ?plot to figure out how to make a title on the figure.
form.i.t = income~(as.numeric(type))
plot(form.i.t)
plot(abline(lm(form.i.t), col = "red"))
# (b) use the plot(), abline(), and lines() functions to create a single scatterplot (just one, not the matrix of them) exactly like the one in row 2 column 1 from the pairs() graph in #7 immediately above. Title this "HW1b scatterplot". Note that you'll need to change the mode of the variable "type" to be numeric on the fly. Also note that pairs() (which only create matrices of scatterplots) and points() (which only adds points to existing plots) won't accomplish what you want. see ?plot to figure out how to make a title on the figure.
form.i.t = income~(as.numeric(type))
plot(form.i.t)
abline(lm(form.i.t)
abline(lm(form.i.t))
#HW Problem 1 	---------------------
# (a) Produce a histogram of the prestige variable that has 5 bins rather than 10. Title this histogram "HW1a histogram"
hist(Duncan$prestige, breaks = 9, main = "HW1a Histogram")
# (b) use the plot(), abline(), and lines() functions to create a single scatterplot (just one, not the matrix of them) exactly like the one in row 2 column 1 from the pairs() graph in #7 immediately above. Title this "HW1b scatterplot". Note that you'll need to change the mode of the variable "type" to be numeric on the fly. Also note that pairs() (which only create matrices of scatterplots) and points() (which only adds points to existing plots) won't accomplish what you want. see ?plot to figure out how to make a title on the figure.
type_numeric <- as.numeric(Duncan$type)
formula.t.i <- income~type_numeric
plot(type_numeric, income, main = "HW1b scatterplot")
abline(lm(formula.t.i),col="red",lwd=2,lty=2)
lines(lowess(formula.t.i),col="darkgreen",lwd=2)
# (c) What is your verbal interpretation for the loess (green) curve for income~type in the plot you did in HW1b (or in row 2 column 1 from the pairs() graph in #7 immediately above if you couldn't do HW1b)? Assign your answer (the character string) to an object "hw1c". You can read information on this dataset using ?Duncan
hw1c <- "After smoothing our line of best fit it is clear that each distinct category has seperate levels of income. Although, it is also clear that income may not necessarily operate as a linear function of (or is linearly predicted by) type of employment. Rather the smoothed line suggests our data may be behaving in a more quadratic manor."
# (d) How do you interpret the linear line (red) for type~income? Assign your answer to hw1d."
hw1d <- "The linear positive trend suggests that as the factors of employment increase, income might as well"
# (e) Run the following code:
my.x <- rnorm(1e4)			      # 1e4 = 10,000 random normal numbers
my.y <- sqrt(.3)*my.x + sqrt(.7)*rnorm(1e4)   # my.y is 60% x (r2 = .6) and 40% random noise
Huge.data <- data.frame(y=my.y,x=my.x)
# It is difficult to 'look' at large amounts of data. Run the following:
Huge.data 		# not helpful!!
plot(Huge.data)
# As you can tell, with large datasets it is difficult to see what is going on. Using the function "sample", figure out a way to plot a random subset (n=300) of the data so that you can see the data pattern better. Title this plot "HW1e scatterplot subset"
plot(Huge.data[sample(nrow(Huge.data), size = 300),])
# (f) Now take four random samples of the data, each of size=300. Plot each in one of the panes of a 2x2 plot. Title each one "HW1f plot X" where X is 1, 2, 3, or 4 (in order). Remember the par(mfrow=) option to change R's default plotting behavior!
par(mfrow=c(2,2))
plot(Huge.data[sample(nrow(Huge.data), size = 300),], main = "HW1f plot 1")
plot(Huge.data[sample(nrow(Huge.data), size = 300),], main = "HW1f plot 2")
plot(Huge.data[sample(nrow(Huge.data), size = 300),], main = "HW1f plot 3")
plot(Huge.data[sample(nrow(Huge.data), size = 300),], main = "HW1f plot 4")
#8 LINEAR REGRESSION	---------------------
# the function "lm" takes in an object of class "formula"
duncan.model.1 <- lm(prestige~income+education)
# notice that your assign the output to be an object (duncan.model.1 in this case)
duncan.model.1
# class is "lm" - automatically assigned by the function "lm"
info(duncan.model.1)
# summary deals with class "lm" as below; both predictors are highly significant: for every $1K gain in income, prestige tends to increase by .5987
summary(duncan.model.1)
# Looking at your data is crucial. Let's use the function we wrote above
MyPairs(Duncan[,2:4])
# check out all the goodies we have access to...
names(duncan.model.1)
#can you figure out how to look at the residuals of the model?
duncan.model.1$residuals			# OK - I'll tell you. This works because mode(duncan.model.1) = list
# what's the most negative residual?
(d.min <- min(duncan.model.1$residuals))
# can you understand this syntax? Shows which profession has the least prestige given its income & educational level.
Duncan[duncan.model.1$residuals==d.min,]
# here are some default methods of graphing the linear model; let's get more specific...
plot(duncan.model.1) #hit your RETURN key IN THE CONSOLE to forward through the various plots
#9 REGRESSION DIAGNOSTICS - ASSESSING NORMALITY	---------------------
# qq.plot is a function from "car" package; are the resids normal?
qqPlot(duncan.model.1$residuals)
# (b) use the plot(), abline(), and lines() functions to create a single scatterplot (just one, not the matrix of them) exactly like the one in row 2 column 1 from the pairs() graph in #7 immediately above. Title this "HW1b scatterplot". Note that you'll need to change the mode of the variable "type" to be numeric on the fly. Also note that pairs() (which only create matrices of scatterplots) and points() (which only adds points to existing plots) won't accomplish what you want. see ?plot to figure out how to make a title on the figure.
type_numeric <- as.numeric(Duncan$type)
formula.t.i <- income~type_numeric
plot(type_numeric, income, main = "HW1b scatterplot")
abline(lm(formula.t.i),col="red",lwd=2,lty=2)
#HW Problem 1 	---------------------
# (a) Produce a histogram of the prestige variable that has 5 bins rather than 10. Title this histogram "HW1a histogram"
hist(Duncan$prestige, breaks = 9, main = "HW1a Histogram")
# (b) use the plot(), abline(), and lines() functions to create a single scatterplot (just one, not the matrix of them) exactly like the one in row 2 column 1 from the pairs() graph in #7 immediately above. Title this "HW1b scatterplot". Note that you'll need to change the mode of the variable "type" to be numeric on the fly. Also note that pairs() (which only create matrices of scatterplots) and points() (which only adds points to existing plots) won't accomplish what you want. see ?plot to figure out how to make a title on the figure.
type_numeric <- as.numeric(Duncan$type)
formula.t.i <- income~type_numeric
plot(type_numeric, income, main = "HW1b scatterplot")
abline(lm(formula.t.i),col="red",lwd=2,lty=2)
par(op)
#HW Problem 1 	---------------------
# (a) Produce a histogram of the prestige variable that has 5 bins rather than 10. Title this histogram "HW1a histogram"
hist(Duncan$prestige, breaks = 9, main = "HW1a Histogram")
# (b) use the plot(), abline(), and lines() functions to create a single scatterplot (just one, not the matrix of them) exactly like the one in row 2 column 1 from the pairs() graph in #7 immediately above. Title this "HW1b scatterplot". Note that you'll need to change the mode of the variable "type" to be numeric on the fly. Also note that pairs() (which only create matrices of scatterplots) and points() (which only adds points to existing plots) won't accomplish what you want. see ?plot to figure out how to make a title on the figure.
type_numeric <- as.numeric(Duncan$type)
formula.t.i <- income~type_numeric
plot(type_numeric, income, main = "HW1b scatterplot")
abline(lm(formula.t.i),col="red",lwd=2,lty=2)
lines(lowess(formula.t.i),col="darkgreen",lwd=2)
getwd()
library(readr)
Prim2008 <- read_csv("Desktop/psyc5541/jrl/hw3/Prim2008.csv")
View(Prim2008)
install.packages("tidyverse")
Duncan$type
library(readxl)
PSYC3111_Fall_2020_Data <- read_excel("Downloads/PSYC3111_Fall_2020_Data.xlsx")
View(PSYC3111_Fall_2020_Data)
data <- read_excel("Downloads/PSYC3111_Fall_2020_Data.xlsx")
View(data)
head(data)
attach(data)
#Mean for attSCALE
mean(attSCALE)
#standard deviation for attSCALE
sd(attSCALE)
#Correlation between AGE and normSCALE
cor.test(age, normsSCALE, alternative="two.sided")
#t-test for pbcSCALE by gender (assume 2-tailed, unpaired)
t.test(pbcSCALE~gender, var.equal=TRUE, paired=F)
#Cronbach's alpha for attitudes_items (all the questions that make up the attitudes scale)
#Search for and install the "psych" library using the 'install' button in the
#bottom right panel. Wait for the library to install. Once it installs type:
library(psych)
#'figure out' the column # of the questions that 'make up' the attitudes scale:
names(PSYC3111_Fall_2020_Data)
#Make a new variable that is 'made up' of only the responses from those
#questions that make up the attitudes scale (i.e. only those columns that start
#with 'att'). We will call it 'attitudes_items':
attitudes_items = data[, 21:27]
#Calculate the Cronbach's Alpha (we want the 'raw alpha')
alpha(attitudes_items)
#age range
range(age)
#average age
mean(age)
#standard deviation of age
sd(age)
#number of males and females
table(gender)
#cronbach's alpha for your scale (or scales)
#'figure out' the column # of the questions that 'make up' the attitudes scale:
names(PSYC3111_Fall_2020_Data)
#cronbach's alpha for your scale (or scales)
#'figure out' the column # of the questions that 'make up' the attitudes scale:
names(data)
#for the attitudes scale
attitudes_items = data[, 21:27]
#for the perceived behavioral control scale
pbc_items = data[, 13:20]
#for the subjective norms scale
norms_items = data[, 7:12]
#for the intentions scale scale
intentions_items = data[, 4:6]
#Calculate the Cronbach's Alpha (we want the 'raw alpha')
alpha(attitudes_items)
#or
alpha(pbc_items)
#if both of your variables are continuous (i.e., if you are running a correlation)
#*note: replace 'variable1 1' and 'variable 2' with your variable names:
mean(variable1)
#Calculate the Cronbach's Alpha (we want the 'raw alpha')
alpha(attitudes_items)
#or
alpha(pbc_items)
#... etc.
alpha(norms_items)
alpha(intentions_items)
subj.info <- read.table('~/Desktop/psyc5541/jrl/final/FinalData/SubjectInfo.txt',header = T);
files <- list.files('~/Desktop/psyc5541/jrl/final/FinalData/')
files <- files[-1]
mat <- matrix(data = NA, nrow = nrow(subj.info), ncol = 52)
for(i in 1:length(files)){
dat.file <- files[i];
num <- substr(dat.file,3,7);
id <- paste("SUB",num,sep = "");
tab <- read.table(paste("FinalData/",dat.file, sep=""));
mat[i,1] <- id;
mat[i,2] <- substr(dat.file,9,9)
mat[i,3:52] <- t(tab$V1);
}
setwd('~/Desktop/psyc5541/jrl/final/')
subj.info <- read.table('~/Desktop/psyc5541/jrl/final/FinalData/SubjectInfo.txt',header = T);
files <- list.files('~/Desktop/psyc5541/jrl/final/FinalData/')
files <- files[-1]
mat <- matrix(data = NA, nrow = nrow(subj.info), ncol = 52)
for(i in 1:length(files)){
dat.file <- files[i];
num <- substr(dat.file,3,7);
id <- paste("SUB",num,sep = "");
tab <- read.table(paste("FinalData/",dat.file, sep=""));
mat[i,1] <- id;
mat[i,2] <- substr(dat.file,9,9)
mat[i,3:52] <- t(tab$V1);
}
df.dirty <- as.data.frame(mat)
df.clean <- df.dirty[!is.na(df.dirty$V1),]
#generate names for our prety new df
header <- vector(mode="logical",length = 50)
for (i in 1:50) {
header[i] <- paste("E",i,sep = "")
}
names(df.clean) <- c("id","group",header)
#merge the two datasets
dat <- merge(subj.info,df.clean,by="id")
dat$group <- as.factor(dat$group)
time <- 1:50
regdat <- matrix(data =NA, nrow = nrow(dat), ncol = 3)
vec1 <- vector(mode="logical", length=nrow(dat))
for (i in 1:nrow(dat)) {
y <- as.numeric(dat[i,5:54])
model <- lm(y~time)
d.vec <- cooks.distance(model)
if(sum(d.vec > .3) > 0){
log.vec <- d.vec > 3
nums <-  names(log.vec[log.vec==T])
cols <- paste("E",nums,sep="")
dat[i,cols] <- NA
}
regdat[i,1] <- dat[i,1]
regdat[i,2] <- model$coefficients[1]
regdat[i,3] <- model$coefficients[2]
}
summary(regdat)
#for q4
regdat <- as.data.frame(regdat)
names(regdat) <- c("id","B0","B1")
#c) Before we get to analyzing the slopes (cognitive incline/decline) and intercepts (starting points), we'd like to visualize a scatterplot for each individual. Create a PDF with as many pages as # of subjects. Each page should be a scatterplot of that subjects' time vs. EF (use dev.off() at the end to stop writing to the pdf() device). Also:
# i) make the points blue if male and red if subject is female
# ii) make the title of each graph be the subject's ID number followed by whether that subject is group A, B, or C
# iii) place the best fit line in each graph, in the red or blue color
# iv) place the text "B0 = xx" and "B1 = xx" in each graph, where xx is the correct intercept or slope. Try to place this in the same location (always in the upper left) of each graph. To do this, try using the vector that comes from the following syntax, which you should run after a plot has been produced
lim <- par('usr')
sub.dat1 <- dat[,c("id","gender","age","group","friends")]
chosen1 <- merge(sub.dat1,regdat,by="id")
chosen1$B0 <- as.numeric(chosen1$B0)
chosen1$B1 <- as.numeric(chosen1$B1)
#question a
model1 <- lm(B1 ~ group, data = chosen1)
#checking for normality
library(car)
quartz()
qqPlot(model1)
qqPlot(model1$residuals)
## all seems well
#cooks d
cooks.d <- cooks.distance(model1)
sum(cooks.d > .3)
chosen1$resid <- residuals(lm(B1 ~ group, data = new.dat))
chosen1$resid <- residuals(lm(B1 ~ group, data = chosen1))
chosen1$resid
col1 <- sample(c(,10),nrow(chosen1),replace=T)
col1 <- sample(c(1,0),nrow(chosen1),replace=T)
col1
col2 <- (col1*-1)+1
?transform
?sample
sample(chosen1$resid,size = nrow(chosen1),replace = T)
sample(chosen1$resid,size = nrow(chosen1),replace = T)
sample(chosen1$resid,size = nrow(chosen1),replace = T)
sample(chosen1$resid,size = nrow(chosen1),replace = T)
chosen1
?anova
tapply(chosen1$resid,chosen1$group,mean)
dat
chosen1$yhat <- predict(lm(B1 ~ group, data = chosen1))
effect <- vector(length = B)
#f) provide the empirical 2-tailed p-values for the two intervention effects using 1000 permutations. Note that this is an example of permuted multiple regressions, so you'll need to get the p-values twice, once for each effect(a total of 2000 permutations), using the residual approach, as we discussed in the bootstrapping/permutation part of the class.
#null dist. is that slopes are not diff across groups
B <- 1000
chosen1$resid <- residuals(lm(B1 ~ group, data = chosen1))
chosen1$yhat <- predict(lm(B1 ~ group, data = chosen1))
effect <- vector(length = B)
chosen1$resid <- residuals(lm(B1 ~ group, data = chosen1))
chosen1$yhat <- predict(lm(B1 ~ group, data = chosen1))
chosen1$resid <- sample(chosen1$resid,size = nrow(chosen1),replace = T)
chosen1$yhat <- sample(chosen1$yhat,size = nrow(chosen1),replace = T)
rand.y <- chosen1$resid + chosen1$yhat
rand.model <- lm(rand.y~chosen1$group)
rand.model$coefficients
rand.y
#mix up resids and Y
chosen1$resid <- sample(chosen1$resid,size = nrow(chosen1),replace = T)
chosen1$yhat <- sample(chosen1$yhat,size = nrow(chosen1),replace = T)
rand.y <- chosen1$resid + chosen1$yhat
rand.y
#mix up resids and Y
chosen1$resid <- sample(chosen1$resid,size = nrow(chosen1),replace = T)
chosen1$yhat <- sample(chosen1$yhat,size = nrow(chosen1),replace = T)
rand.y <- chosen1$resid + chosen1$yhat
rand.model <- lm(rand.y~chosen1$group)
for(i in 1:B){
#mix up resids and Y
chosen1$resid <- sample(chosen1$resid,size = nrow(chosen1),replace = T)
chosen1$yhat <- sample(chosen1$yhat,size = nrow(chosen1),replace = T)
rand.y <- chosen1$resid + chosen1$yhat
rand.model <- lm(rand.y~chosen1$group)
effects.B.perm[i] <- rand.model$coefficients[2]
effects.C.perm[i] <- rand.model$coefficients[3]
}
effects.B.perm <- vector(length = B)
effects.C.perm <- vector(length = B)
for(i in 1:B){
#mix up resids and Y
chosen1$resid <- sample(chosen1$resid,size = nrow(chosen1),replace = T)
chosen1$yhat <- sample(chosen1$yhat,size = nrow(chosen1),replace = T)
rand.y <- chosen1$resid + chosen1$yhat
rand.model <- lm(rand.y~chosen1$group)
effects.B.perm[i] <- rand.model$coefficients[2]
effects.C.perm[i] <- rand.model$coefficients[3]
}
effects.B.perm
effects.C.perm
rand.model$coefficients
chosen1$yhat
?anova
tapply(chosen1$resid,chosen1$group,mean)
tapply(chosen1$resid,chosen1$group,mean)[2]
tapply(chosen1$resid,chosen1$group,mean)[2]-tapply(chosen1$resid,chosen1$group,mean)[1]
table(chosen1$group)
chosen1$resid
residuals(lm(chosen1$B1~chosen1$group))
chosen1$resid <- residuals(lm(B1 ~ group, data = chosen1))
chosen1$resid
tapply(chosen1$resid,chosen1$group,mean)
#mix up resids and Y
chosen1$resid <- sample(chosen1$resid,size = nrow(chosen1),replace = T)
chosen1$resid
avgs <- tapply(chosen1$resid,chosen1$group,mean)
avgs[1]
#f) provide the empirical 2-tailed p-values for the two intervention effects using 1000 permutations. Note that this is an example of permuted multiple regressions, so you'll need to get the p-values twice, once for each effect(a total of 2000 permutations), using the residual approach, as we discussed in the bootstrapping/permutation part of the class.
#null dist. is that diff in means of resids between baseline and treatment are not diff across groups
B <- 1000
chosen1$resid <- residuals(lm(B1 ~ group, data = chosen1))
chosen1$yhat <- predict(lm(B1 ~ group, data = chosen1))
effects.B.perm <- vector(length = B)
effects.C.perm <- vector(length = B)
for(i in 1:B){
#mix up resids and Y
chosen1$resid <- sample(chosen1$resid,size = nrow(chosen1),replace = T)
avgs <- tapply(chosen1$resid,chosen1$group,mean)
effects.B.perm[i] <- avgs[2]-avgs[1]
effects.C.perm[i] <- avgs[3]-avgs[1]
}
effects.B.perm
hist(effects.B.perm)
quartz()
hist(effects.B.perm)
hist(effects.C.perm)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
runif(1)
hist(effects.B.perm)
quartz()
quartz()
quartz()
